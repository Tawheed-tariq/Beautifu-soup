{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling an entire site\n",
    "While crawling the entire site we might find one link may of the times in the page.To avoid crawling the same page twice, it is extremely important that all internal links discovered are formatted consistently, and kept in a running set for easy lookups, while the program is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = set()\n",
    "# def getlinks(pageUrl):\n",
    "#     html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
    "#     bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "#     for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "#         if link.attrs['href'] not in pages:\n",
    "#             #we have found a new page\n",
    "#             newPage = link.attrs['href']\n",
    "#             print(newPage)\n",
    "#             pages.add(newPage)\n",
    "#             getlinks(newPage) #recursively run this function to get all the links of this page\n",
    "\n",
    "# getlinks('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a default recursion limit (the number of times a program can recursively call itself) of 1,000. Because Wikipedia’s network of links is extremely large, this program will eventually hit that recursion limit and stop, unless you put in a recursion counter or something to prevent that from happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = set()\n",
    "def getlinks(pageUrl):\n",
    "    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
    "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text()) #title of the page\n",
    "        print(bs.find(id ='mw-content-text').find_all('p')[0].text) #first paragraph of page\n",
    "        # print(bs.find(id='ca-edit').find('span').find('a').attrs['href']) #edit links\n",
    "    except AttributeError:\n",
    "        print('page not found! continuing')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if link.attrs['href'] not in pages:\n",
    "            #we have found a new page\n",
    "            newPage = link.attrs['href']\n",
    "            print('-'*20)\n",
    "            print(newPage)\n",
    "            pages.add(newPage)\n",
    "            getlinks(newPage) #recursively run this function to get all the links of this page\n",
    "\n",
    "getlinks('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling redirects\n",
    "Redirects allow a web server to point one domain name or URL to a piece of content\n",
    "at a different location. There are two types of redirects:<br>\n",
    "• Server-side redirects, where the URL is changed before the page is loaded <br>\n",
    "• Client-side redirects, sometimes seen with a “You will be redirected in 10 seconds” type of message, where the page loads before redirecting to the new one <br>\n",
    "<br>\n",
    "With server-side redirects, you usually don’t have to worry. If you’re using the urllib\n",
    "library with Python 3.x, it handles redirects automatically! If you’re using the requests\n",
    "library, make sure to set the allow-redirects flag to True:\n",
    "<br>\n",
    "`r = requests.get('http://github.com', allow_redirects=True)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
